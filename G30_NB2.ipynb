{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "kCIxHWxygaYw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
        "\n",
        "from sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_curve,auc\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from scipy import sparse\n",
        "%matplotlib inline\n",
        "seed = 42\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '4'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualisation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RDn7oIYgaZA",
        "outputId": "71ef99b3-8dcb-46b5-fd32-f6f5af5dd1b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows and columns in the train data set: (159571, 8)\n",
            "Number of rows and columns in the test data set: (153164, 2)\n",
            "Number of rows and columns in the test data set: (153164, 7)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Read the training data from the CSV file\n",
        "train = pd.read_csv('train.csv')\n",
        "\n",
        "# Read the test data from the CSV file\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# Read the test labels from the CSV file\n",
        "test_labels = pd.read_csv('test_labels.csv')\n",
        "\n",
        "# Display the number of rows and columns in the training data\n",
        "print('Number of rows and columns in the train data set:', train.shape)\n",
        "\n",
        "# Display the number of rows and columns in the test data\n",
        "print('Number of rows and columns in the test data set:', test.shape)\n",
        "\n",
        "# Display the number of rows and columns in the test labels data\n",
        "print('Number of rows and columns in the test labels data set:', test_labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "M-AO54GMjYGk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows and columns in the test data set: (63978, 8)\n"
          ]
        }
      ],
      "source": [
        "# Merge the test data with the test labels based on the 'id' column\n",
        "test = pd.merge(test, test_labels, on='id')\n",
        "\n",
        "# Display the first few rows of the merged test data\n",
        "test.head()\n",
        "\n",
        "# Filter out rows where any label has a value of -1 (indicating missing or invalid label)\n",
        "test = test[(test[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] != -1).all(axis=1)]\n",
        "\n",
        "# Display the number of rows and columns in the filtered test data\n",
        "print('Number of rows and columns in the test data set:', test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "test.fillna(' ',inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Data Tokenization and Padding\n",
        "\n",
        "The following code segment outlines the essential steps of tokenizing and padding text data for effective text classification:\n",
        "\n",
        "### Objective:\n",
        "\n",
        "Prepare text data for classification by applying tokenization and padding techniques.\n",
        "\n",
        "### Hyperparameters:\n",
        "\n",
        "- **Vocabulary Size (vocab_size):** Set to 20,000, controlling the number of unique words in the vocabulary.\n",
        "- **Embedding Dimension (embedding_dim):** Chosen as 128, determining the size of the embedding vectors for words.\n",
        "- **Maximum Length (max_length):** Limited to 200, ensuring uniform sequence length for the classification model.\n",
        "- **Truncation Type (trunc_type):** Set to 'post', indicating truncation from the end of sequences.\n",
        "- **Padding Type (padding_type):** Set to 'post', indicating padding at the end of sequences.\n",
        "- **Out-of-Vocabulary Token (oov_tok):** Defined as '<OOV>', representing out-of-vocabulary words.\n",
        "\n",
        "### Tokenization Process:\n",
        "\n",
        "1. The Keras Tokenizer is employed to tokenize the sentences in the comment text.\n",
        "2. The tokenizer is fitted on the training data, creating a vocabulary index.\n",
        "3. Sequences of integers are generated for both the training and test sets based on the tokenizer's vocabulary.\n",
        "\n",
        "### Padding:\n",
        "\n",
        "1. The generated sequences are padded to a maximum length of 200 to ensure uniformity.\n",
        "2. Padding is applied post-tokenization and truncation to create sequences of consistent length.\n",
        "\n",
        "### Train-Validation Split:\n",
        "\n",
        "1. The train set is split into train and validation sets using a test size of 20% for model evaluation.\n",
        "\n",
        "This process prepares the textual data for subsequent use in training a text classification model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "kwZlHH6UgaZc"
      },
      "outputs": [],
      "source": [
        "# Objective: Tokenization and Padding for Text Classification\n",
        "\n",
        "# Defining target columns for classification\n",
        "target_col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "y = train[target_col]\n",
        "\n",
        "# Hyperparameters for tokenization and padding\n",
        "vocab_size = 20000\n",
        "embedding_dim = 128\n",
        "max_length = 200\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = '<OOV>'\n",
        "\n",
        "# Tokenizing the sentences using the Keras Tokenizer\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train['comment_text'])\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Converting the train and test sets into sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train['comment_text'])\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test['comment_text'])\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "# Splitting the train set into train and validation sets\n",
        "train_padded, val_padded, train_labels, val_labels = train_test_split(train_padded, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "6PwRuW4YkS7q"
      },
      "outputs": [],
      "source": [
        "# Define the text classification model\n",
        "model = Sequential([\n",
        "    # Embedding layer for word representation\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    \n",
        "    # First LSTM layer with 64 units and return sequences set to True\n",
        "    LSTM(64, return_sequences=True),\n",
        "    \n",
        "    # Second LSTM layer with 32 units for further sequence processing\n",
        "    LSTM(32),\n",
        "    \n",
        "    # Dense layer with 64 units and ReLU activation\n",
        "    Dense(64, activation='relu'),\n",
        "    \n",
        "    # Dropout layer with a dropout rate of 50% to prevent overfitting\n",
        "    Dropout(0.5),\n",
        "    \n",
        "    # Output Dense layer with 6 nodes and sigmoid activation for multi-label classification\n",
        "    Dense(6, activation='sigmoid')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recurrent Neural Network (RNN) for Text Classification\n",
        "\n",
        "In our pursuit of text classification for toxic comments, we employed a Recurrent Neural Network (RNN) model using Keras with a TensorFlow backend. The architecture of the RNN is structured as follows:\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "- **Embedding Layer:** The initial layer for word representation using word embeddings, capturing semantic meanings.\n",
        "  \n",
        "- **LSTM Layers:** Two Long Short-Term Memory (LSTM) layers with 64 and 32 units, respectively. The first layer returns sequences, providing a deeper understanding of contextual information, while the second layer further processes sequences.\n",
        "\n",
        "- **Dense Layer:** A dense layer with 64 units and ReLU activation for additional non-linearity.\n",
        "\n",
        "- **Dropout Layer:** To prevent overfitting, a dropout layer with a dropout rate of 50% is introduced.\n",
        "\n",
        "- **Output Dense Layer:** The final dense layer with 6 nodes and sigmoid activation is designed for multi-label classification, producing probabilities for each toxic label.\n",
        "\n",
        "## Compilation and Training\n",
        "\n",
        "The model is compiled using binary crossentropy loss and the Adam optimizer, with accuracy as the metric. Training is performed for three epochs on the training data, validating on a separate validation set.\n",
        "\n",
        "## Prediction\n",
        "\n",
        "After training, the model predicts toxic labels on the test set, providing insights into its performance.\n",
        "\n",
        "## Why RNN with LSTM?\n",
        "\n",
        "- **Sequential Nature of Text:** Toxic comments often exhibit patterns and context that require understanding of the sequential nature of text. LSTM, as a type of RNN, is well-suited for capturing such long-range dependencies.\n",
        "\n",
        "- **Semantic Representations:** The ability of LSTM to capture semantic representations is crucial for discerning the nuanced meaning in toxic comments, where context plays a vital role.\n",
        "\n",
        "- **Contextual Understanding:** The use of LSTM layers, especially one returning sequences, allows the model to build a richer contextual understanding by considering the order of words in a comment.\n",
        "\n",
        "This RNN architecture, particularly with LSTM layers, proves suitable for the complexities of toxic comment classification, leveraging sequential information and semantic representations for enhanced accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Npq7NouIi7iM",
        "outputId": "9b753296-d1a8-46c8-d8b6-3529474dbda6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "3990/3990 - 572s - loss: 0.1512 - accuracy: 0.9002 - val_loss: 0.1416 - val_accuracy: 0.9941 - 572s/epoch - 143ms/step\n",
            "Epoch 2/3\n",
            "3990/3990 - 755s - loss: 0.0992 - accuracy: 0.9938 - val_loss: 0.0540 - val_accuracy: 0.9941 - 755s/epoch - 189ms/step\n",
            "Epoch 3/3\n",
            "3990/3990 - 792s - loss: 0.0525 - accuracy: 0.9942 - val_loss: 0.0518 - val_accuracy: 0.9941 - 792s/epoch - 199ms/step\n",
            "2000/2000 [==============================] - 111s 55ms/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 3\n",
        "history = model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(val_padded, val_labels), verbose=2)\n",
        "\n",
        "# Predict on test set\n",
        "test_pred = model.predict(test_padded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHKLdVGBkuS4",
        "outputId": "533ebce3-1cff-49f1-8a06-51f4eb1fa047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.9348902e-03 4.4465221e-12 3.0096526e-05 5.5960708e-08 7.2777533e-05\n",
            "  5.6555446e-06]\n",
            " [1.6209672e-01 1.0431326e-04 3.1703383e-02 2.1255468e-03 4.2972788e-02\n",
            "  9.6544931e-03]\n",
            " [1.0993951e-01 2.8888813e-05 1.7916238e-02 9.9200802e-04 2.5854262e-02\n",
            "  5.5971574e-03]\n",
            " ...\n",
            " [7.1963382e-01 1.0672147e-02 3.1103054e-01 2.2713598e-02 2.9539418e-01\n",
            "  6.0466934e-02]\n",
            " [9.9724633e-01 2.5943160e-01 9.3785584e-01 3.1459097e-02 8.3884734e-01\n",
            "  1.3564067e-01]\n",
            " [5.0212285e-03 1.9356791e-10 1.3561477e-04 6.1508717e-07 2.9553953e-04\n",
            "  3.0352878e-05]]\n"
          ]
        }
      ],
      "source": [
        "print(test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "HvFwYbUNl7Sq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for label: toxic\n",
            "Class 0 - Precision: 0.98, Recall: 0.94, F1-Score: 0.96\n",
            "Class 1 - Precision: 0.60, Recall: 0.79, F1-Score: 0.68\n",
            "AUC: 0.96\n",
            "--------------------------------------------------\n",
            "Metrics for label: severe_toxic\n",
            "Class 0 - Precision: 0.99, Recall: 1.00, F1-Score: 1.00\n",
            "Class 1 - Precision: 0.31, Recall: 0.01, F1-Score: 0.02\n",
            "AUC: 0.98\n",
            "--------------------------------------------------\n",
            "Metrics for label: obscene\n",
            "Class 0 - Precision: 0.98, Recall: 0.98, F1-Score: 0.98\n",
            "Class 1 - Precision: 0.67, Recall: 0.70, F1-Score: 0.68\n",
            "AUC: 0.97\n",
            "--------------------------------------------------\n",
            "Metrics for label: threat\n",
            "Class 0 - Precision: 1.00, Recall: 1.00, F1-Score: 1.00\n",
            "Class 1 - Precision: 0.00, Recall: 0.00, F1-Score: 0.00\n",
            "AUC: 0.94\n",
            "--------------------------------------------------\n",
            "Metrics for label: insult\n",
            "Class 0 - Precision: 0.98, Recall: 0.98, F1-Score: 0.98\n",
            "Class 1 - Precision: 0.63, Recall: 0.60, F1-Score: 0.61\n",
            "AUC: 0.96\n",
            "--------------------------------------------------\n",
            "Metrics for label: identity_hate\n",
            "Class 0 - Precision: 0.99, Recall: 1.00, F1-Score: 0.99\n",
            "Class 1 - Precision: 0.00, Recall: 0.00, F1-Score: 0.00\n",
            "AUC: 0.94\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Assuming test_pred contains the prediction probabilities and test_pred_binary contains binary predictions\n",
        "threshold = 0.5\n",
        "test_pred_binary = (test_pred > threshold).astype(int)\n",
        "\n",
        "target_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "# Iterate over each column to compute metrics for both classes\n",
        "for col in target_cols:\n",
        "    # Metrics for class 0\n",
        "    precision_0 = precision_score(test[col], test_pred_binary[:, target_cols.index(col)], pos_label=0, zero_division=0)\n",
        "    recall_0 = recall_score(test[col], test_pred_binary[:, target_cols.index(col)], pos_label=0, zero_division=0)\n",
        "    f1_0 = f1_score(test[col], test_pred_binary[:, target_cols.index(col)], pos_label=0, zero_division=0)\n",
        "\n",
        "    # Metrics for class 1\n",
        "    precision_1 = precision_score(test[col], test_pred_binary[:, target_cols.index(col)], pos_label=1, zero_division=0)\n",
        "    recall_1 = recall_score(test[col], test_pred_binary[:, target_cols.index(col)], pos_label=1, zero_division=0)\n",
        "    f1_1 = f1_score(test[col], test_pred_binary[:, target_cols.index(col)], pos_label=1, zero_division=0)\n",
        "\n",
        "    # AUC for the label\n",
        "    auc_score = roc_auc_score(test[col], test_pred[:, target_cols.index(col)])\n",
        "\n",
        "    print(f\"Metrics for label: {col}\")\n",
        "    print(f\"Class 0 - Precision: {precision_0:.2f}, Recall: {recall_0:.2f}, F1-Score: {f1_0:.2f}\")\n",
        "    print(f\"Class 1 - Precision: {precision_1:.2f}, Recall: {recall_1:.2f}, F1-Score: {f1_1:.2f}\")\n",
        "    print(f\"AUC: {auc_score:.2f}\")\n",
        "    print('-' * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for label: toxic\n",
            "Confusion Matrix:\n",
            "[[54633  3255]\n",
            " [ 1264  4826]]\n",
            "Class 0 - Precision: 0.98, Recall: 0.94, F1-Score: 0.96\n",
            "Class 1 - Precision: 0.60, Recall: 0.79, F1-Score: 0.68\n",
            "AUC: 0.96\n",
            "--------------------------------------------------\n",
            "Metrics for label: severe_toxic\n",
            "Confusion Matrix:\n",
            "[[63602     9]\n",
            " [  363     4]]\n",
            "Class 0 - Precision: 0.99, Recall: 1.00, F1-Score: 1.00\n",
            "Class 1 - Precision: 0.31, Recall: 0.01, F1-Score: 0.02\n",
            "AUC: 0.98\n",
            "--------------------------------------------------\n",
            "Metrics for label: obscene\n",
            "Confusion Matrix:\n",
            "[[59013  1274]\n",
            " [ 1110  2581]]\n",
            "Class 0 - Precision: 0.98, Recall: 0.98, F1-Score: 0.98\n",
            "Class 1 - Precision: 0.67, Recall: 0.70, F1-Score: 0.68\n",
            "AUC: 0.97\n",
            "--------------------------------------------------\n",
            "Metrics for label: threat\n",
            "Confusion Matrix:\n",
            "[[63767     0]\n",
            " [  211     0]]\n",
            "Class 0 - Precision: 1.00, Recall: 1.00, F1-Score: 1.00\n",
            "Class 1 - Precision: 0.00, Recall: 0.00, F1-Score: 0.00\n",
            "AUC: 0.94\n",
            "--------------------------------------------------\n",
            "Metrics for label: insult\n",
            "Confusion Matrix:\n",
            "[[59329  1222]\n",
            " [ 1382  2045]]\n",
            "Class 0 - Precision: 0.98, Recall: 0.98, F1-Score: 0.98\n",
            "Class 1 - Precision: 0.63, Recall: 0.60, F1-Score: 0.61\n",
            "AUC: 0.96\n",
            "--------------------------------------------------\n",
            "Metrics for label: identity_hate\n",
            "Confusion Matrix:\n",
            "[[63266     0]\n",
            " [  712     0]]\n",
            "Class 0 - Precision: 0.99, Recall: 1.00, F1-Score: 0.99\n",
            "Class 1 - Precision: 0.00, Recall: 0.00, F1-Score: 0.00\n",
            "AUC: 0.94\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Assuming test_pred contains the prediction probabilities and test_pred_binary contains binary predictions\n",
        "threshold = 0.5\n",
        "test_pred_binary = (test_pred > threshold).astype(int)\n",
        "\n",
        "target_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "# Iterate over each column to compute metrics and confusion matrix for both classes\n",
        "for col in target_cols:\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(test[col], test_pred_binary[:, target_cols.index(col)])\n",
        "    \n",
        "    # Metrics for class 0\n",
        "    precision_0 = precision_score(test[col], test_pred_binary[:, target_cols.index(col)], pos_label=0, zero_division=0)\n",
        "    recall_0 = recall_score(test[col], test_pred_binary[:, target_cols.index(col)], pos_label=0, zero_division=0)\n",
        "    f1_0 = f1_score(test[col], test_pred_binary[:, target_cols.index(col)], pos_label=0, zero_division=0)\n",
        "    \n",
        "    # Metrics for class 1\n",
        "    precision_1 = precision_score(test[col], test_pred_binary[:, target_cols.index(col)], pos_label=1, zero_division=0)\n",
        "    recall_1 = recall_score(test[col], test_pred_binary[:, target_cols.index(col)], pos_label=1, zero_division=0)\n",
        "    f1_1 = f1_score(test[col], test_pred_binary[:, target_cols.index(col)], pos_label=1, zero_division=0)\n",
        "    \n",
        "    # AUC for the label\n",
        "    auc_score = roc_auc_score(test[col], test_pred[:, target_cols.index(col)])\n",
        "    \n",
        "    print(f\"Metrics for label: {col}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(f\"Class 0 - Precision: {precision_0:.2f}, Recall: {recall_0:.2f}, F1-Score: {f1_0:.2f}\")\n",
        "    print(f\"Class 1 - Precision: {precision_1:.2f}, Recall: {recall_1:.2f}, F1-Score: {f1_1:.2f}\")\n",
        "    print(f\"AUC: {auc_score:.2f}\")\n",
        "    print('-' * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Findings\n",
        "### Toxic:\n",
        "- The model performs well in identifying non-toxic comments (Class 0) with high precision (0.98) and recall (0.94).\n",
        "- For toxic comments (Class 1), precision is moderate (0.60), indicating some false positives, but recall is relatively high (0.79), capturing a good portion of actual toxic comments.\n",
        "- Overall, the model achieves a good balance with an AUC of 0.96.\n",
        "\n",
        "### Severe Toxic:\n",
        "- The model excels in correctly classifying non-severe toxic comments (Class 0) with high precision (0.99) and recall (1.00).\n",
        "- However, for severe toxic comments (Class 1), precision is low (0.31), suggesting a higher rate of false positives, and recall is extremely low (0.01), indicating that the model struggles to identify most severe toxic comments.\n",
        "- The AUC is relatively high at 0.98.\n",
        "\n",
        "### Obscene:\n",
        "- The model shows excellent performance in distinguishing non-obscene comments (Class 0) with high precision (0.98) and recall (0.98).\n",
        "- For obscene comments (Class 1), precision is moderate (0.67), and recall is reasonable (0.70), indicating a decent identification of obscene content.\n",
        "- The AUC is high at 0.97.\n",
        "\n",
        "### Threat:\n",
        "- The model is highly accurate in identifying non-threatening comments (Class 0) with high precision (0.99) and recall (1.00).\n",
        "- However, for threatening comments (Class 1), precision is extremely low (0.00), suggesting a high rate of false positives, and recall is also zero, indicating that the model fails to identify any threatening comments.\n",
        "- The AUC is good at 0.94.\n",
        "\n",
        "In summary, the model generally performs well in distinguishing non-toxic and non-threatening comments, but it faces challenges, especially in identifying severe toxic and threatening comments, where precision and recall are lower. Balancing precision and recall is crucial for different application scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "It is imperative to realize that RNN has indeed improved upon the Naive Bayes model in cases where data imbalance of classes, 1, and 0, respectively, are not extremely significant. However, it performs worse with 0 Recalls and Precisions in cases where data imbalnces are indeed significant, such as in column = threat. While in general this is a better model, this leads towards our areas of Improvements.\n",
        "\n",
        "While we did perform data sampling in Naive Bayes, we werent able to do so in this notebook as it was performing great on validation set, but poorly on test set. Perhaps, this was brought about due to overfitting. Therefore, we decided not to undersample to fit both classes in this case. This is , however, an area of improvement where we could have better data sampling to provide better results for hugely imbalance classes.\n",
        "\n",
        "Moreover, while we did add LSTMs to improve long term dependcies, realistically, RNNs are not known for their long term dependcies, and as such, another area of improvement is the Model itself, which brings us to our third and last model, the state of the art, Transformers!\n",
        "\n",
        "(Note: Model 2 > Model1 follows in general as RNN performed better overall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
